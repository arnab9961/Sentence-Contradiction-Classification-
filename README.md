# ğŸ“Œ Sentence Contradiction Classification

## **Introduction**  
The goal of this project is to classify pairs of sentences as **"contradiction," "entailment," or "neutral"** based on their meaning. The task requires building a model that can understand semantic relationships between text pairs.

---

## **ğŸ“‚ 1. Reading the Dataset**  
The dataset contains **multiple sentence pairs** with labels indicating their relationship.  

ğŸ‘‰ **Filtered the dataset to include only English sentences.**  
ğŸ‘‰ **Removed missing and duplicate values.**  

---

## **ğŸ“Š 2. Exploratory Data Analysis (EDA)**  
Before training the model, we analyzed the data:  
ğŸ‘‰ **Checked label distribution to see how many contradiction, entailment, and neutral cases exist.**  
ğŸ‘‰ **Visualized word distributions using graphs.**  
ğŸ‘‰ **Generated statistical summaries of numerical features.**  

---

## **ğŸ” 3. Text Preprocessing Pipeline**  
ğŸ‘‰ **Removing HTML tags**  
ğŸ‘‰ **Removing Punctuations**  
ğŸ‘‰ **Performing Lemmatization**  
ğŸ‘‰ **Performing Lowercase**  
ğŸ‘‰ **Removing Stopwords**  
ğŸ‘‰ **Decontracting words**  
ğŸ‘‰ **Replacing special characters with their string symbols**  

---

## **âš™ï¸ 4. Feature Engineering**  
ğŸ‘‰ **Added key sentence-level features:**  
  - `sentence1_len`, `sentence2_len`, `common_word_count`, `word_overlap_ratio`  
ğŸ‘‰ **Applied TF-IDF (Term Frequency-Inverse Document Frequency) vectorization.**  
ğŸ‘‰ **Extracted word embeddings to enhance semantic understanding.**  

---

## **ğŸ¤– 5. Model Building & Training**  

The dataset was split into:  
ğŸ‘‰ **80% Training Data**  
ğŸ‘‰ **20% Test Data**  

We tested multiple machine learning models:  
ğŸ‘‰ **Logistic Regression**  
ğŸ‘‰ **Random Forest**  
ğŸ‘‰ **XGBoost (XGB)**  
ğŸ‘‰ **Support Vector Machine (SVM)**  
ğŸ‘‰ **Artificial Neural Network (ANN) using Keras**  
ğŸ‘‰ **LSTM for sequence-based learning**  
ğŸ‘‰ **BERT for advanced NLP understanding**  

We also performed **Grid Search** for **hyperparameter tuning**.  

---

## **ğŸ“ˆ 6. Model Evaluation**  

ğŸ‘‰ **XGB performed the best among traditional models, achieving 45.12% accuracy.**  
ğŸ‘‰ **BERT outperformed all models, achieving an accuracy of 71.25%.**  
ğŸ‘‰ A **confusion matrix** was used to analyze model errors.  

---

## **ğŸš€ 7. Deployment**  
We deployed the model using **Streamlit**, allowing users to:  
ğŸ‘‰ Enter **two sentences**  
ğŸ‘‰ Get a **classification result (Contradiction, Entailment, or Neutral)**  


